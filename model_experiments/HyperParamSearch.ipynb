{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKjt0U2gHO-d",
        "outputId": "c92f6cd7-8e47-4965-ff19-a3486ed39830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: ray[tune] in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (23.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.31.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.5.3)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (9.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2023.6.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.31.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.13.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->ray[tune]) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install -U \"ray[tune]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3sZyIsUPJjd"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iV_4VHYlGPgG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "from ray import tune\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_VGLf0WPJje"
      },
      "source": [
        "# Loading prepared train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "65walMT8PJje"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('train_data.csv')\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "train_data['overview'].fillna('', inplace=True)\n",
        "test_data['overview'].fillna('', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TYxDfFI6HKNw"
      },
      "outputs": [],
      "source": [
        "# 2. Dataset and Dataloader\n",
        "class RevenueDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data, device, max_length=256):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.data = data\n",
        "        self.original_language_cols = [x for x in data.columns if x.startswith('original_language_')]\n",
        "        self.genre_cols = [x for x in data.columns if x.startswith('genre_')]\n",
        "        self.cast_cols = [x for x in data.columns if x.startswith('cast_')]\n",
        "        self.crew_cols = [x for x in data.columns if x.startswith('crew_')]\n",
        "        self.device = device\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        inputs = self.tokenizer.encode_plus(row['overview'], add_special_tokens=True, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt').to(self.device)\n",
        "\n",
        "        original_language = torch.tensor(row[self.original_language_cols].values.astype(float), dtype=torch.float, device=self.device)\n",
        "        genres = torch.tensor(row[self.genre_cols].values.astype(float), dtype=torch.float, device=self.device)\n",
        "        cast = torch.tensor(row[self.cast_cols].values.astype(float), dtype=torch.float, device=self.device)\n",
        "        crew = torch.tensor(row[self.crew_cols].values.astype(float), dtype=torch.float, device=self.device)\n",
        "        budget = torch.tensor(row['budget_100M'], dtype=torch.float, device=self.device)\n",
        "        budget_unknown = torch.tensor(row['budget_unknown'], dtype=torch.float, device=self.device)\n",
        "        revenue = torch.tensor(row['revenue_100M'], dtype=torch.float, device=self.device)\n",
        "\n",
        "        x = torch.cat((\n",
        "            inputs[\"input_ids\"].squeeze(),\n",
        "            inputs[\"attention_mask\"].squeeze(),\n",
        "            original_language,\n",
        "            genres,\n",
        "            cast,\n",
        "            crew,\n",
        "            budget.unsqueeze(0),\n",
        "            budget_unknown.unsqueeze(0)\n",
        "        ))\n",
        "\n",
        "        return x, revenue\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "92Gl23DDPJjr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13375718-3d24-4091-bd3d-c4846cbaf505"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LuYEQv4qPJjr"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SNwmOgfTH9g1"
      },
      "outputs": [],
      "source": [
        "train_dataset = RevenueDataset(tokenizer, train_data, DEVICE)\n",
        "test_dataset = RevenueDataset(tokenizer, test_data, DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TsGtrV-wkOXJ"
      },
      "outputs": [],
      "source": [
        "NUM_GENRES = len(train_dataset.genre_cols)\n",
        "NUM_CAST = len(train_dataset.cast_cols)\n",
        "NUM_CREW = len(train_dataset.crew_cols)\n",
        "NUM_ORIGINAL_LANGUAGES = len(train_dataset.original_language_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xom76OmSIAAa"
      },
      "outputs": [],
      "source": [
        "# 3. Model\n",
        "class RevenuePredictor(nn.Module):\n",
        "    def __init__(self, bert_embedding_size = 128, original_language_embedding_size = 32, cast_embedding_size = 32, crew_embedding_size = 32, hidden_size = 256):\n",
        "        super(RevenuePredictor, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "        # Linear layer for textual embeddings\n",
        "        self.linear_overview = nn.Linear(self.bert.config.hidden_size, bert_embedding_size)\n",
        "\n",
        "        # Linear layer for original language embeddings\n",
        "        self.linear_original_language = nn.Linear(NUM_ORIGINAL_LANGUAGES, original_language_embedding_size)\n",
        "\n",
        "        # Linear layer for embedding cast\n",
        "        self.linear_cast = nn.Linear(NUM_CAST, cast_embedding_size)\n",
        "\n",
        "        # Linear layer for embedding crew\n",
        "        self.linear_crew = nn.Linear(NUM_CREW, crew_embedding_size)\n",
        "\n",
        "        # Budget and budget_unknown, and genres\n",
        "        self.other_features_size = 2 + NUM_GENRES\n",
        "\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(bert_embedding_size + original_language_embedding_size + cast_embedding_size + crew_embedding_size + self.other_features_size, hidden_size),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        bert_out = self.bert(input_ids=input[:, :256].long(), attention_mask=input[:, 256:512].long())\n",
        "        overview_embedding = self.linear_overview(bert_out['pooler_output'])\n",
        "\n",
        "        original_language_embedding = self.linear_original_language(input[:, 512:512+NUM_ORIGINAL_LANGUAGES])\n",
        "        cast_embedding = self.linear_cast(input[:, 512+NUM_ORIGINAL_LANGUAGES:512+NUM_ORIGINAL_LANGUAGES+NUM_CAST])\n",
        "        crew_embedding = self.linear_crew(input[:, 512+NUM_ORIGINAL_LANGUAGES+NUM_CAST:512+NUM_ORIGINAL_LANGUAGES+NUM_CAST+NUM_CREW])\n",
        "        other_features = input[:, 512+NUM_ORIGINAL_LANGUAGES+NUM_CAST+NUM_CREW:]\n",
        "\n",
        "\n",
        "        return self.output_layer(torch.cat((\n",
        "            overview_embedding,\n",
        "            original_language_embedding,\n",
        "            cast_embedding,\n",
        "            crew_embedding,\n",
        "            other_features\n",
        "        ), dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BWqZlp_dhz5i"
      },
      "outputs": [],
      "source": [
        "def raytune_objective(config):\n",
        "    batch_size = config['batch_size']\n",
        "\n",
        "    train_dataloader = DataLoader(config['train_dataset'], batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(config['test_dataset'], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    NUM_EPOCHS = 10\n",
        "\n",
        "    model = RevenuePredictor(\n",
        "        bert_embedding_size=config['bert_embedding_size'],\n",
        "        original_language_embedding_size=config['original_language_embedding_size'],\n",
        "        cast_embedding_size=config['cast_embedding_size'],\n",
        "        crew_embedding_size=config['crew_embedding_size'],\n",
        "        hidden_size=config['hidden_size']\n",
        "    )\n",
        "    optimizer = AdamW(model.parameters(), lr=config['adamw_lr'])\n",
        "    loss_fn = nn.MSELoss()\n",
        "    SCHEDULER_STEPS = NUM_EPOCHS * len(train_dataloader)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=250, num_training_steps=SCHEDULER_STEPS)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    best_mse = float('inf')\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(\"Started epoch \", epoch + 1)\n",
        "        model.train()\n",
        "        for x, y in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            predictions = model(x)\n",
        "            loss = loss_fn(predictions, y.unsqueeze(1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # loop.set_postfix(mseloss=loss.item())\n",
        "\n",
        "        # Evaluate on test set\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        actuals = []\n",
        "        with torch.no_grad():\n",
        "            for x, y in test_dataloader:\n",
        "                predictions = model(x)\n",
        "                preds.extend(predictions.squeeze().tolist())\n",
        "                actuals.extend(y.tolist())\n",
        "        mse = loss_fn(torch.tensor(preds).unsqueeze(1), torch.tensor(actuals).unsqueeze(1)).item()\n",
        "        print(\"MSE on test set after epoch \", epoch + 1, \": \", mse)\n",
        "        history.append(mse)\n",
        "        if mse < best_mse:\n",
        "            best_mse = mse\n",
        "    print(history)\n",
        "    # Plot loss history\n",
        "    plt.plot(history)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('MSE Loss')\n",
        "    plt.title('Loss curve')\n",
        "    plt.show()\n",
        "    return {'best_mse': best_mse}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WIMaO7Dghz5j"
      },
      "outputs": [],
      "source": [
        "search_space = {\n",
        "    'batch_size': tune.choice([8, 16, 32, 64]),\n",
        "    'bert_embedding_size': tune.choice([64, 128, 256]),\n",
        "    'original_language_embedding_size': tune.choice([16, 32, 64]),\n",
        "    'cast_embedding_size': tune.choice([16, 32, 64]),\n",
        "    'crew_embedding_size': tune.choice([16, 32, 64]),\n",
        "    'hidden_size': tune.choice([128, 256]),\n",
        "    'adamw_lr': tune.loguniform(5e-5, 1e-3),\n",
        "    'train_dataset': train_dataset,\n",
        "    'test_dataset': test_dataset\n",
        "}\n",
        "\n",
        "tuner = tune.Tuner(\n",
        "    tune.with_resources(\n",
        "        tune.with_parameters(raytune_objective),\n",
        "        resources={\"cpu\": 2, \"gpu\": 1}\n",
        "    ),\n",
        "    param_space=search_space, tune_config=tune.TuneConfig(num_samples=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tune_results = tuner.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixoTl0Wci3tU",
        "outputId": "0fad410c-87a4-4792-e83d-f9299cb7eec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-11-30 19:08:26,903\tINFO tune.py:595 -- [output] This will use the new output engine with verbosity 1. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------+\n",
            "| Configuration for experiment     raytune_objective_2023-11-30_19-08-25   |\n",
            "+--------------------------------------------------------------------------+\n",
            "| Search algorithm                 BasicVariantGenerator                   |\n",
            "| Scheduler                        FIFOScheduler                           |\n",
            "| Number of trials                 1                                       |\n",
            "+--------------------------------------------------------------------------+\n",
            "\n",
            "View detailed results here: /root/ray_results/raytune_objective_2023-11-30_19-08-25\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/raytune_objective_2023-11-30_19-08-25`\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}